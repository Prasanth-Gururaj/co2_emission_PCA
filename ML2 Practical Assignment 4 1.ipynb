{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa25ae2-0429-4ba5-a8c6-2dcf51a41c5b",
   "metadata": {},
   "source": [
    "# EXPLORING GLOBAL GREENHOUSE GAS EMISSION PATTERNS WITH UNSUPERVISED MACHINE LEARNING\n",
    "Group Members : Sanjay Ramesh Kannan, Prasanth Gururaj, Michelle Rhea Sathish "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34da27-fb21-480e-9f65-df19d9b6fd01",
   "metadata": {},
   "source": [
    "### INTRODUCTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13285b65-a520-4dfa-812d-a05436575e8d",
   "metadata": {},
   "source": [
    "One of the most concerning issues of this century is Global Warming and understanding its contributing factors can help implement effective solutions. The different Greenhouse gases such as Carbon Dioxide (CO2), methane and nitrous oxide are emitted significantly, varying across countries depending on the energy sources, population and industrial activity.\n",
    "\n",
    "Our goal here is to find out how countries differ based on their greenhouse gas emissions from the year 2000 to 2022 and based on these differences we can discover the underlying patterns of this outcome.\n",
    "\n",
    "To achieve this goal ,we use Unsupervised Machine Learning techniques such as Principal Component Analysis(PCA) , Singular Value Decomposition (SVD), K-means Clustering and Hierarchical Clustering.\n",
    "\n",
    "To analyze and explore the patterns of CO2 and Green House Gas emissions across Countries ,we used the [CO2 and Greenhouse Gas Emissions Data](https://ourworldindata.org/co2-and-greenhouse-gas-emissions) sourced from [Our World in Data](https://data.worldbank.org). \n",
    "The original dataset was huge — over 50000 rows and 79 columns — with everything from country level CO₂ emissions to broader climate metrics. It had raw totals like CO₂ from coal, oil, and gas, but also more nuanced stats like emissions per person, methane levels, and even estimated temperature change linked to national emissions.A lot of it was interesting, but not all of it was useful for pattern detection.\n",
    "So, we trimmed it down. Using the variable definitions from the [Codebook](https://github.com/owid/co2-data/blob/master/owid-co2-codebook.csv), we pulled together a streamlined set of features that help answer our question, that includes:\n",
    "* Demographics (GDP, year)\n",
    "* Emission sources (coal, oil, gas, cement, and more)\n",
    "* CO₂ intensity (per capita, per GDP)\n",
    "* Global contribution (share of global CO₂)\n",
    "* Climate impact metrics (methane, nitrous oxide, total GHGs, temperature change)\n",
    "\n",
    "From this, we created a clean working dataset — [Filtered_CO2.csv](https://github.com/Prasanth-Gururaj/co2_emission_PCA/blob/main/Filtered_CO2.csv) — with just the indicators that matter most. That gave us a solid foundation for analysis between 2000 and 2022.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02dab2-a0a3-4fdd-8057-6957ef10a7ac",
   "metadata": {},
   "source": [
    "### THEORETICAL BACKGROUND:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2f09f-a907-4f7d-9c3b-79dd32c0c21b",
   "metadata": {},
   "source": [
    "#### What is Unsupervised Machine Learning?\n",
    "\n",
    "Unsupervised Machine Learning (UML) is a branch of ML that deals with unlabelled data. Unlike supervised ML, where the goal is to predict or create patterns from a known or labelled output, Unsupervised ML explores the input data and understands its structure with no such guidance.\n",
    "\n",
    "In Unsupervised ML, only feature variables are present in the data with no target variables from which the algorithms discover the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd56105-1da8-4614-aaaf-74d33aa47e77",
   "metadata": {},
   "source": [
    "#### Types of Unsupervised Learning Algorithms:\n",
    "\n",
    "1. **Dimensionality Reduction**  \n",
    "2. **Clustering**\n",
    "\n",
    "### 1. Dimensionality Reduction\n",
    "\n",
    "Dimensionality reduction techniques reduce the number of input features while preserving as much important information as possible. In high-dimensional datasets, many features may be irrelevant, redundant, or just noisy. These techniques help compress the data into a lower-dimensional space where patterns are easier to observe — all without predefined labels.\n",
    "\n",
    "#### Common Algorithms for Dimensionality Reduction:\n",
    "\n",
    "**a) Principal Component Analysis (PCA):**  \n",
    "PCA linearly combines original features into principal components (axes) that capture the maximum variance in the data.\n",
    "\n",
    "**Working Principle:**\n",
    "\n",
    "- Standardize the data  \n",
    "- Compute the covariance matrix:  \n",
    "  $$\n",
    "  \\Sigma = \\frac{1}{n} X^T X\n",
    "  $$\n",
    "- Compute eigenvectors and eigenvalues of $\\Sigma$  \n",
    "- Sort the eigenvalues and retain the top *k* values  \n",
    "- Project data onto the top *k* eigenvectors  \n",
    "- The result is reduced data with uncorrelated features capturing the highest variance.\n",
    "\n",
    "**b) Singular Value Decomposition (SVD):**  \n",
    "SVD is a matrix factorization technique closely related to PCA.A powerful linear algebra tool that decomposes the standardized data into components ranked by how much “energy” or structure they capture. After pre-scaling the data, SVD decomposes $X$ as:\n",
    "\n",
    "$$\n",
    "X = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $U$: left singular vectors  \n",
    "- $\\Sigma$: diagonal matrix of singular values  \n",
    "- $V^T$: right singular vectors (transposed)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "### 2) Clustering\n",
    "\n",
    "Clustering is the task of grouping similar data points into clusters, where points in a cluster are more similar to each other and dissimilar to those in other clusters.\n",
    "\n",
    "#### Common Clustering Algorithms:\n",
    "\n",
    "**K-Means Clustering:**  \n",
    "- First, it randomly initializes *k* centroids.  \n",
    "- Assigns each point to the nearest centroid using the Euclidean distance.  \n",
    "- Updates each centroid as the average of points assigned to it.  \n",
    "- Iterates this process until centroids no longer change significantly.\n",
    "\n",
    "The objective function minimized by K-Means is:\n",
    "\n",
    "$$\n",
    "\\min \\sum_{i=1}^{K} \\sum_{x_j \\in C_i} \\| x_j - \\mu_i \\|^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $K$ = number of clusters  \n",
    "- $C_i$ = cluster $i$  \n",
    "- $x_j$ = data point  \n",
    "- $\\mu_i$ = centroid of cluster $i$\n",
    "\n",
    "\n",
    "**Hierarchical Clustering:**  \n",
    "Hierarchical clustering builds a tree-like structure (dendrogram) that shows how points are grouped step-by-step from individuals to bigger clusters.\n",
    "\n",
    "There are two types:\n",
    "\n",
    "- **1) Agglomerative (Bottom-Up):**  \n",
    "  Starts with each point as its own cluster.  \n",
    "  Calculates distances between all clusters using Euclidean distance.  \n",
    "  At each step, merges the two closest clusters.  \n",
    "  Recalculates distances and repeats until all points form one big cluster.\n",
    "\n",
    "- **2) Divisive (Top-Down):**  \n",
    "  Starts with one big cluster.  \n",
    "  Repeatedly splits clusters until each point is its own cluster.\n",
    "\n",
    "\n",
    "**Dendrogram:**  \n",
    "A tree-like diagram that shows which clusters were merged and at what distance.  \n",
    "It can be cut at a chosen height to retain a specific number of clusters.\n",
    "\n",
    "\n",
    "**Linkage Methods:**  \n",
    "The distance between clusters is measured by linkage methods:  \n",
    "\n",
    "- **Single:** Closest point between two clusters  \n",
    "- **Complete:** Farthest point between two clusters  \n",
    "- **Average:** Average distance between all points in the two clusters  \n",
    "- **Ward:** Increase in variance after merging clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c237a-41c7-4789-bf9b-976eb4751fd8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Performance Assessment Metrics\n",
    "\n",
    "1) **PCA**  \n",
    "- Use a **Scree Plot** or **Explained Variance Ratio** to determine how many principal components should be retained.  \n",
    "- The goal is to retain enough components to capture the majority of the variance in the data.\n",
    "\n",
    "2) **SVD**  \n",
    "- Evaluate the **Retained Variance** by calculating the proportion of total singular value \"energy\" that is captured.  \n",
    "- This helps in understanding how much information is preserved in the reduced representation.\n",
    "\n",
    "3) **K-Means Clustering**  \n",
    "- **Inertia**: The within-cluster sum of squares, indicating how compact the clusters are. Lower is better.  \n",
    "- **Silhouette Score**: Measures how similar a point is to its own cluster compared to other clusters (range: -1 to 1). Higher is better.\n",
    "\n",
    "4) **Hierarchical Clustering**  \n",
    "- **Dendrogram Visualization**: Visual inspection helps identify a good number of clusters.  \n",
    "- **Cophenetic Correlation Coefficient**: Assesses how well the dendrogram reflects the actual pairwise distances between data points.                       A higher coefficient means better preservation of distance structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe3637-6fe3-4e4b-a940-3ffc694d9604",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "1) **PCA**: It is sensitive to scaling and outliers. Components are not very interpretable in the real world.  \n",
    "2) **SVD**: Computationally more intensive for large datasets.  \n",
    "3) **K-means**: Sensitive to scale and outliers. Not suitable for non-convex clusters.  \n",
    "4) **Hierarchical**: Not scalable to very large datasets. Sensitive to noise and outliers. Does not have the ability to reassign points once merged.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5935fbd4-c46c-44cc-9a7c-3c665bc44c9a",
   "metadata": {},
   "source": [
    "\n",
    "### METHODOLOGY:\n",
    "\n",
    "**Data Cleaning and Processing:**  \n",
    "We started by loading our filtered dataset of 2287 rows and 17 columns and dropped the country column since it’s not something clustering algorithms can work with directly.  \n",
    "Next, we removed rows that were completely empty and noted how many we lost in the process. We also dropped any column with more than 50 percent missing values, since those were too sparse to trust.  \n",
    "For the rest, we filled in missing values using column averages to avoid throwing away good data. Then we scaled everything using z-score normalization — bringing all features to the same scale so no single variable would dominate the analysis because of its units or range.  \n",
    "With a clean and standardized dataset, we were ready to dig into clustering and uncover meaningful patterns.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5f7e4-e4f6-48f5-91ad-9c935a683208",
   "metadata": {},
   "source": [
    "### Unsupervised Machine Learning Techniques:\n",
    "\n",
    "#### 1) Singular Value Decomposition:\n",
    "\n",
    "**a) Analyzes the Variance and Importance of components:**  \n",
    "Once we had the cleaned and scaled emissions dataset ready, we were trying to understand the underlying patterns in the emission profiles of countries.  \n",
    "Using Singular Value Decomposition (SVD), we directly factorize the data matrix and capture the most important patterns in emissions using just a few components.\n",
    "\n",
    "The standardized matrix X was decomposed as:  \n",
    "$$\n",
    "X = U \\cdot \\Sigma \\cdot V^T\n",
    "$$\n",
    "\n",
    "This factorization breaks the data into:  \n",
    "- **U**: Country-level coefficients  \n",
    "- **Σ**: Diagonal matrix of component strengths (singular values)  \n",
    "- **V^T**: Feature directions (loadings)  \n",
    "\n",
    "Using this decomposition, we projected the data into a lower-dimensional space by retaining only the top k components that capture the most variance.\n",
    "\n",
    "---\n",
    "\n",
    "**b) Dimensionality Reduction and Reconstruction Error:**  \n",
    "Further we are trying to see how well different numbers of components could reconstruct the original data by computing the relative reconstruction error. \n",
    "By varying k, we analyze how well the top-k components reconstruct the original data.  \n",
    "The **Frobenius norm error** between scaled X and its rank-k approximation quantifies the reconstruction error.\n",
    "\n",
    "---\n",
    "\n",
    "**c) Matrix Completion of Missing Data:**  \n",
    "Next we explored Matrix Completion using truncated SVD—an especially useful technique when dealing with datasets that have missing entries. \n",
    "For each missing-data fraction and each truncation rank M,iteratively refines the reconstruction.  \n",
    "Each iteration calculated the mean squared error (MSE) on the observed values and checked how much it improved from the previous iteration.  \n",
    "Once the improvement was negligible, the loop stopped.\n",
    "\n",
    "For the matrix reconstruction task, we used:  \n",
    "- **Mean squared error (MSE)**: on observed entries, to check how well the approximation matched the actual data.\n",
    "\n",
    "For component selection, we observed:  \n",
    "- **Explained variance plots**: to decide where the elbow point occurred.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b30f57-4b41-420a-9303-ddb1b0c36bee",
   "metadata": {},
   "source": [
    "#### 2) K-Means Clustering:\n",
    "\n",
    "Once we had the top 8 SVD components, we moved on to clustering.  \n",
    "Ran K-Means clustering across a range of cluster counts (K = 2 to 10), storing the inertia and silhouette scores for each configuration.  \n",
    "These two metrics helped us evaluate how compact and well-separated the clusters were.\n",
    "\n",
    "Used the elbow method on the inertia plot to find a point that gave the appropriate number of clusters.  \n",
    "Used the silhouette score to see which cluster yielded the most naturally coherent grouping.  \n",
    "Visualized the clusters in 2D using the first two SVD components.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Hierarchical Clustering:\n",
    "\n",
    "Using the standardized data we ran the Hierarchical clustering comparing with the 4 Linkage Methods for the Dendrograms:  \n",
    "**Ward, Complete, Average, and Single**  \n",
    "\n",
    "The dendrogram was cut at 5 Clusters itself.  \n",
    "Evaluated using the Silhouette Score to assess cluster quality.  \n",
    "Visualized the dendrogram for interpretability.  \n",
    "Chose the Best Identified Clustering method and compared with K-Means Clustering  \n",
    "Scatter plots showed both clusterings on the SVD-reduced space.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa94f5d-6b89-4468-afba-a0afef945a06",
   "metadata": {},
   "source": [
    "RESULTS: \n",
    "1) SVD :\n",
    "These plots show how much each SVD component “weighs” in your data:\r\n",
    "\r\n",
    "Left: the share of total singular‐value mass (σₖ/∑σᵢ), with the first component alone contributing about 30 % and the next few rapidly declining.\r\n",
    "\r\n",
    "Right: the analogous variance‐explained (σₖ²/∑σᵢ²), where the first component captures nearly 60 % of the variance, the second ~14 %, the third ~11 %, etc., implying that just the top 3–5 components recover most of the structure.\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotSVD_2025-05-27_011443.\n",
    "Figure 1 : a) Relative Singular-Value Contributions b) Individual Explained Variance per Componentpng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0298b5-d81d-4286-af47-f7a40e12a4fa",
   "metadata": {},
   "source": [
    "![Alt text](ML2_Assignment_Images/ScreenshotSVD_2025-05-27_011443.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c09135-7e31-4915-89b2-7d21e4d27a6c",
   "metadata": {},
   "source": [
    "### RESULTS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7637c4-8463-42c6-8743-7c1c96d553a1",
   "metadata": {},
   "source": [
    "### SVD:\n",
    "\n",
    "**1. Relative Singular-Value Contributions**\n",
    "\n",
    "These plots show how much each SVD component “weighs” in our data.\n",
    "\n",
    "- **Plot i):** The share of total singular‐value mass (σₖ/∑σᵢ), with the first component alone contributing about 30% and the next two add another 26%, the next few rapidly declining.\n",
    "\n",
    "- **Plot ii):** The analogous variance‐explained (σₖ²/∑σᵢ²), where the first component captures nearly 60% of the variance, the second ~14%, the third ~11%, etc., implying that just the top 3–5 components recover most of the structure.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotSVD_2025-05-27_011443.png)  \n",
    "**Figure 1:** *i) Relative Singular-Value Contributions  ii) Individual Explained Variance per Component*\n",
    "\n",
    "\n",
    "\n",
    "**Visualizing the Singular Value Energy Drop-off**\n",
    "\n",
    "This figure plots each component’s share of the total singular‐value “energy” in descending order.  \n",
    "The first component alone accounts for roughly 30%, the second about 15%, and by the fifth it’s below 6%, indicating that most of the signal is captured by just the first few components.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotRelativeSVD_2025-05-27_011630.png)  \n",
    "**Figure 2:** *Relative Contributions of Singular Values*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72ffdd-6c05-46f2-85dc-8bfc952633a0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2. Individual Explained Variance (PVE):**\n",
    "\n",
    "Figure 3 shows how the total variance in standardized emission data accumulates as more SVD components are included.  \n",
    "Component 1 alone captures **58%** of the variance, Component 2 captures **14%**, and Component 3 captures **11%**.  \n",
    "By Component 4, about **90%** of the variance is explained, and by Component 8 it's up near **98%**.  \n",
    "This confirms that a low‐dimensional embedding (e.g., **k = 5–8**) captures nearly all the meaningful structure in the full 17-feature space.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPVE_2025-05-27_011727.png)  \n",
    "**Figure 3:** *Cumulative Proportional Variance Explained Curve*\n",
    "\n",
    "\n",
    "\n",
    "**3. Reconstruction Error vs. Rank:**\n",
    "\n",
    "Figure 4 illustrates how well the original standardized data matrix can be approximated by reconstructing it using only the top **k** singular values and vectors.  \n",
    "- With **k = 1**, the reconstruction error is nearly **60%**.  \n",
    "- With **k = 4**, the error drops to about **30%**.  \n",
    "- With **k = 10**, the error reduces to only **4%**.\n",
    "\n",
    "This shows that **k between 5 to 8** offers a good trade-off between complexity and accuracy.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotErrorRank_2025-05-27_011819.png)  \n",
    "**Figure 4:** *Reconstruction Error vs Rank k*\n",
    "\n",
    "\n",
    "\n",
    "**4. Matrix Completion with RMSE:**\n",
    "\n",
    "When randomly masking the data and reconstructing with truncated SVD:  \n",
    "- **Rank M = 3** gives the **lowest RMSE** across all missing data fractions (from 5% to 30%).  \n",
    "- Ranks greater than about **4** tend to **overfit** the observed entries and perform **worse on the missing values**.\n",
    "\n",
    "This supports the idea that lower-rank approximations (M = 3–4) are more robust for matrix completion.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotRMSE_2025-05-27_011905.png)  \n",
    "**Figure 5:** *CO₂ Matrix Completion Using Truncated SVD*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ad961-5c68-41a5-8030-ed66591856ea",
   "metadata": {},
   "source": [
    "### K-MEANS CLUSTERING\n",
    "\n",
    "#### Cluster Assessments\n",
    "\n",
    "1) **Elbow Method**  \n",
    "The optimal number of clusters is likely 5, since increasing *k* beyond this point doesn’t significantly reduce WCSS. Choosing more clusters would overcomplicate the model without meaningful gain.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotClusterWCSS_2025-05-27_012011.png)  \n",
    "**Figure 6:** *WCSS vs Number of Clusters*\n",
    "\n",
    "2) **Silhouette Score**  \n",
    "The optimal number of clusters is *k = 5*. It offers the best trade-off between cohesion and separation, with the highest silhouette score among the tested values. Clustering beyond this point introduces fragmentation without meaningful structure.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotSilhouette_2025-05-27_012125.png)  \n",
    "**Figure 7:** *Silhouette Score vs Number of Clusters*\n",
    "\n",
    "#### SVD Space – Clusters\n",
    "\n",
    "The SVD plot reveals 5 well-formed clusters in reduced-dimension space. Most clusters are compact and distinct, validating the choice of *k = 5*. Component 1 appears to play a bigger role in separating the green and red clusters, while Component 2 helps differentiate orange and purple.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotSVDSpace_2025-05-27_012211.png)  \n",
    "**Figure 8:** *Clusters in SVD Space*\n",
    "\n",
    "#### Cluster Membership\n",
    "\n",
    "The vast majority of observations belong to Cluster 0, showing strong stability in emission patterns for most countries over time. The smaller clusters (1–4) capture specific or exceptional profiles that are rare and consistent year-to-year, rather than emerging or disappearing trends.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotMember_2025-05-27_031211.png)  \n",
    "**Figure 9:** *Clusters Membership over Time*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6492c3b-f85b-4f00-a5d8-252a11b867e1",
   "metadata": {},
   "source": [
    "\n",
    "### HIERARCHICAL CLUSTERING\n",
    "\n",
    "#### Dendrograms\n",
    "\n",
    "Complete and Average linkage provide the best clustering structure based on silhouette scores (0.75 and 0.72).  \n",
    "Ward linkage, while commonly used, performs poorly here (0.35) — possibly due to unbalanced cluster sizes.  \n",
    "Single linkage does okay (0.69), but beware of chaining and over-merged clusters.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotDendro_2025-05-27_031313.png)  \n",
    "**Figure 10:** Dendrograms for Different Linkage Methods\n",
    "\n",
    "#### K-Means vs Hierarchical Clustering\n",
    "\n",
    "Both methods identify similar global structure (with 5 main groupings), but they differ in boundary definition and assignment.  \n",
    "• K-Means creates compact, evenly distributed clusters, ideal for spherical groupings.  \n",
    "• Complete Linkage emphasizes separation, resulting in more elongated or irregular clusters — especially when groups differ in density or shape.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotCompare_2025-05-27_031358.png)  \n",
    "**Figure 11:** Comparison Plots of K-Means and Complete Hierarchical Clustering\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073283f5-fba1-4ed7-88ce-c64d8d85b18d",
   "metadata": {},
   "source": [
    "#### COUNTRIES VS EMISSION PATTERNS\n",
    "\n",
    "These cluster labels provide a policy-relevant and interpretable summary of how countries differ in their emission patterns:\n",
    "\n",
    "• Clusters 1 and 4 represent wealthier, high-emission countries — but for different reasons (fossil fuel reliance vs. overall per-capita consumption).  \n",
    "• Cluster 2 highlights countries managing emissions effectively, often with strong energy policies.  \n",
    "• Clusters 3 and 5 capture developing economies heavily reliant on coal and cement — possibly different in scale or region.\n",
    "\n",
    "| Cluster | Label                   | Key Traits                                         | Countries (Examples)                         |\n",
    "|---------|-------------------------|----------------------------------------------------|----------------------------------------------|\n",
    "| 1       | Oil & Gas Dependent     | High per-capita emissions from oil/gas             | Qatar                                        |\n",
    "| 2       | Low-Emission Economies  | Low emissions, efficient energy use                | India, Japan, Germany, Russia, Brazil        |\n",
    "| 3       | Coal & Cement Economies | High coal/cement CO₂, developing economies         | China                                        |\n",
    "| 4       | High Per-Capita Emitters| High CO₂/capita, high GDP                          | United States                                |\n",
    "| 5       | Coal & Cement Economies | High coal/cement CO₂, developing economies         | China                                        |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0471d0-86eb-40dd-8450-cda0aa10a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA :\n",
    "\n",
    "These two plots shown in Figure 12 and Figure 13 shows the number of components in which majority of the data can be retained without the use of all the features.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCAVarExp_2025-05-27 052117.png)\n",
    "Figure 12: Variance Explained by Component\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCACumul_2025-05-27 052242.png)\n",
    "Figure 13: Cumulative Variance Explained\n",
    "\n",
    "\n",
    "Observing Figures 14 and Figure 15 :The PCA plot (PC1 vs PC2) shows clear separation between country clusters, especially along PC1 — capturing major variation like GDP and emission levels. You can actually see structure emerging.\n",
    "But in the raw CO2 vs GDP plot, clusters are more spread out and less distinct — harder to tell apart visually. That’s because raw features can be correlated or skewed, while PCA rotates the space to highlight meaningful variance. So PCA helps reveal patterns that aren’t obvious in the original data.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCA2Compo2025-05-27 052441.png)\n",
    "Figure 14: PCA : First 2 Principal Components\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCACO2GDP2025-05-27 052518.png)\n",
    "Figure 15: CO2 vs GDP - Orihginal Feature Plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4d6e2-7c14-4c48-8037-7a7777e2e3d4",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "These two plots shown in Figure 12 and Figure 13 show the number of components in which the majority of the data can be retained without the use of all the features.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCAVarExp_2025-05-27_052117.png)  \n",
    "Figure 12: Variance Explained by Component\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCACumul2025-05-27_052242.png)  \n",
    "Figure 13: Cumulative Variance Explained\n",
    "\n",
    "Observing Figures 14 and 15: The PCA plot (PC1 vs PC2) shows clear separation between country clusters, especially along PC1 — capturing major variation like GDP and emission levels. You can actually see structure emerging.  \n",
    "But in the raw CO₂ vs GDP plot, clusters are more spread out and less distinct — harder to tell apart visually. That’s because raw features can be correlated or skewed, while PCA rotates the space to highlight meaningful variance. So PCA helps reveal patterns that aren’t obvious in the original data.\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCA2Compo2025-05-27_052441.png)  \n",
    "Figure 14: PCA: First 2 Principal Components\n",
    "\n",
    "![Alt text](ML2_Assignment_Images/ScreenshotPCACO2GDP2025-05-27_052518.png)  \n",
    "Figure 15: CO₂ vs GDP – Original Feature Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ad527-d6f9-4888-9976-1a414a4a40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCUSSION :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50223b3f-62b6-4144-82c6-b8a6dfcca4ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### DISCUSSION:\n",
    "\n",
    "Our results showed clear structure in country-level emission patterns using SVD and Clustering, while also highlighting some limitations and directions for future exploration.\n",
    "\n",
    "\n",
    "\n",
    "#### Key Takeaways\n",
    "\n",
    "**SVD:**\n",
    "- The top 3 components explained approximately 80% of the total variance.\n",
    "- Most of the structure was captured early, making SVD highly effective for dimensionality reduction.\n",
    "\n",
    "**Matrix Completion:**\n",
    "- A rank-3 reconstruction gave the lowest RMSE across all levels of missing data.\n",
    "- Higher ranks began to overfit the observed data, worsening performance on missing entries.\n",
    "\n",
    "**Clustering:**\n",
    "- **Complete Linkage** had the highest silhouette score (0.754) and formed intuitive clusters (e.g., coal-heavy nations, low-emission groups).\n",
    "- **K-Means** also performed well (silhouette score = 0.657), and there was strong agreement between both methods (Adjusted Rand Index = 0.849).\n",
    "\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Some missing or incomplete data may have skewed clustering and reconstruction accuracy.\n",
    "- Clustering outcomes were sensitive to the number of retained SVD components.\n",
    "- K-Means assumes spherical clusters, possibly missing non-linear or irregular shapes in the data.\n",
    "\n",
    "\n",
    "\n",
    "#### Future Work\n",
    "\n",
    "- Experiment with non-linear dimensionality reduction techniques such as t-SNE and UMAP.\n",
    "- Explore more flexible clustering algorithms like DBSCAN or Gaussian Mixture Models (GMM).\n",
    "- Integrate additional features such as population, renewable energy usage, or economic indicators to provide richer contextual insight.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4cbbb-475e-40b3-99e2-8260a6285a5d",
   "metadata": {},
   "source": [
    "\n",
    "### CONCLUSION:\n",
    "\n",
    "#### Broader Impacts and Real-World Use\n",
    "\n",
    "This project shows that greenhouse gas emission patterns across countries are not random but structured, and a few core factors explain most of the variations. Using SVD, we found that just the top 3–5 components capture most of the signal. This allows for significant simplification of the data without sacrificing key insights.\n",
    "\n",
    "In the real world, this kind of structure is valuable:\n",
    "\n",
    "- Policy makers can group countries with similar emission profiles to design more targeted and fair climate strategies.\n",
    "- Global organizations like the UN or World Bank can prioritize action by identifying which countries share similar energy or industrial footprints.\n",
    "- Analysts and researchers can build faster, more efficient models by focusing only on the most informative dimensions.\n",
    "\n",
    "The clustering results add another clear layer of interpretability. Distinct groups emerged, such as oil-heavy economies, coal-driven industrial countries, and low-emission efficient nations. These clusters can guide international negotiations and foster meaningful cooperation.\n",
    "\n",
    "Overall, this work demonstrates how unsupervised machine learning can transform complex environmental data into actionable insights that support smarter, more equitable global climate decisions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd0538-73a6-47a8-8ad5-f2112271772f",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. **Our World in Data. (2024). CO₂ and Greenhouse Gas Emissions Dataset.**  \n",
    "   Retrieved from [https://github.com/owid/co2-data](https://github.com/owid/co2-data)  \n",
    "   - Our Primary Dataset, providing CO₂ and greenhouse gas emissions by country and year.\n",
    "\n",
    "\n",
    "2. **Ritchie, H., Roser, M., & Rosado, P. (2020). CO₂ and Greenhouse Gas Emissions.**  \n",
    "   Our World in Data. Retrieved from [https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions](https://ourworldindata.org/co2-and-other-greenhouse-gas-emissions)  \n",
    "   - Contextual and explanatory resource on emissions trends and causes.\n",
    "\n",
    "\n",
    "3. **Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: A review and recent developments.**  \n",
    "   *Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences*, 374(2065), 20150202.  \n",
    "   - Key reference for the PCA methodology used in dimensionality reduction and feature analysis.\n",
    "\n",
    "\n",
    "4. **MacQueen, J. (1967). Some Methods for Classification and Analysis of Multivariate Observations.**  \n",
    "   In *Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability* (Vol. 1, pp. 281–297). University of California Press.  \n",
    "   - Original paper proposing the k-means clustering algorithm.\n",
    "\n",
    "\n",
    "5. **Rokach, L., & Maimon, O. (2005). Clustering methods.**  \n",
    "   In *Data Mining and Knowledge Discovery Handbook* (pp. 321–352). Springer.  \n",
    "   - Comprehensive overview of clustering techniques, including hierarchical and centroid-based methods.\n",
    "\n",
    "\n",
    "6. **IPCC. (2023). Climate Change 2023: Synthesis Report.**  \n",
    "   Intergovernmental Panel on Climate Change. Retrieved from [https://www.ipcc.ch/report/ar6/syr/](https://www.ipcc.ch/report/ar6/syr/)  \n",
    "   - Provides background on the global climate impact and relevance of greenhouse gas emissions data.\n",
    "\n",
    "\n",
    "7. **Kaufman, L., & Rousseeuw, P. J. (2009). Finding Groups in Data: An Introduction to Cluster Analysis.**  \n",
    "   Wiley.  \n",
    "   - Foundational text for evaluating clustering results like silhouette scores and interpreting cluster quality.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe62acb-acf9-40f4-93ac-417f704e31af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
